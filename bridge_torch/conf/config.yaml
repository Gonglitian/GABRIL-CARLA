defaults:
  # Choose algorithm preset (BC only after refactor)
  - algo: bc
  # Choose dataset preset: lift_carrot_100 | pull_pot_100 | ... (see conf/bridgedata/*)
  - bridgedata: lift_carrot_100
  - _self_

# Saliency 统一配置（单一入口）
# - enabled: 是否启用注意力（Reg）正则
# - weight:  正则损失的权重（总损失 = base + weight * reg）
# - beta:    生成注意力掩码时的温度（见 get_gaze_mask）
# - alpha:   数据侧时间聚合系数（obs_horizon>1 时，s(t-k)*alpha^k 累加）
saliency:
  enabled: false
  weight: 5
  beta: 1.0
  alpha: 1.0

# Experiment naming (used for save_dir/run folder name prefix)
# Override on CLI: name=myexp
name: ""

# Debug mode (disables W&B online if your WandB wrapper uses it)
debug: false

# Paths
# - data_path: root containing BDV2 numpy folders (task/train/out.npy, task/val/out.npy)
# - save_dir: where to store runs and checkpoints
data_path: "/data3/vla-reasoning/dataset/bdv2_numpy"
save_dir: "/data3/vla-reasoning/torch_runs"

# Training schedule
# Override examples:
# - batch_size=512 num_steps=100000
# - eval_batches=null     # disable evaluation entirely
batch_size: 2000
val_batch_size: 200
num_steps: 5000
log_interval: 100
eval_interval: 100
eval_batches: 0   # <=0 means unlimited; null to skip
save_interval: 5000
# resume_path: "/vla-reasoning/torch_runs/bc_lift_carrot_100/ckpt_2000000.pt"

# Model/optimizer/scheduler defaults are defined under algo/bc.yaml and merged here.

# Runtime
# device: cuda | cpu
seed: 42
device: cuda

# DDP (launch with torchrun; Hydra multirun is separate)
# - enabled: true to wrap model in DDP when launched via torchrun
# - find_unused_parameters: enable only if required by your model graph
ddp:
  enabled: true
  find_unused_parameters: false

# AMP: Automatic Mixed Precision
# - dtype: bf16 | fp16 (bf16 recommended on A100/H100; fp16 uses GradScaler)
amp:
  enabled: true
  dtype: bf16
  growth_factor: 2.0
  backoff_factor: 0.5
  growth_interval: 2000

# torch.compile optimization (PyTorch 2.x)
# - kwargs.mode: default | reduce-overhead | max-autotune
# - kwargs.dynamic/fullgraph: advanced toggles for dynamo
compile:
  enabled: true
  kwargs:
    mode: default
    dynamic: false
    fullgraph: false

# DataLoader settings (used by bridge_numpy iterator wrapper)
dataloader:
  num_workers: 8
  prefetch_factor: 2
  persistent_workers: true
  pin_memory: true


# Scheduler selector consumed inside agents
# - type: constant | warmup_cosine
scheduler:
  type: warmup_cosine
  # Default total decay horizon ties to total num_steps unless overridden by algo/bc.yaml
  decay_steps: ${num_steps}

# WandB project (set wandb.enabled=false to disable fully)
wandb:
  project: bridgedata_torch
  enabled: true


# Inherited YAML override examples:
# - Change encoder backbone:  encoder_kwargs.arch=resnet50
# - Turn on proprio:          model.use_proprio=true
# - Enable saliency:          saliency.enabled=true
# - Adjust saliency strength: saliency.weight=0.2 saliency.beta=1.0 saliency.alpha=0.7
# - Override data knobs:      data.obs_horizon=2 data.augment=true

# Hydra runtime tweaks (keep PWD; no outputs to ./multirun by default)
hydra:
  job:
    chdir: false
  run:
    dir: .
