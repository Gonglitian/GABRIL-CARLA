# Algo: State-only Behavior Cloning (no images)

# Model/Policy
model:
  hidden_dims: [256, 256, 256]
  dropout_rate: 0.1
  tanh_squash_distribution: false
  state_dependent_std: false
  fixed_std: [1, 1, 1, 1, 1, 1, 1]
  use_proprio: true

# Optimizer
optimizer:
  lr: 0.0003
  weight_decay: 0.0

# Scheduler (decay_steps defaulted to ${num_steps} in root)
scheduler:
  type: warmup_cosine
  decay_steps: 0 # set to num_steps in train_hydra.py
  warmup_steps: 0 # set to 0.1*num_steps in train_hydra.py

# Data settings (torch Dataset)
data:
  goal_relabeling_strategy: uniform
  goal_relabeling_kwargs:
    reached_proportion: 0.0
  relabel_actions: true
  augment: false  # no images; keep false
  obs_horizon: 1
  act_pred_horizon: 1
  saliency_alpha: ${saliency.alpha}
  shuffle_buffer_size: 25000
  augment_next_obs_goal_differently: false
  augment_kwargs: {}

# Encoder placeholder (not used for state-only)
encoder: none